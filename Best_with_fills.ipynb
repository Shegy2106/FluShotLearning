{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVWI3lvaYNIO",
    "outputId": "639b1f79-a525-4b4a-d223-6c8d109858c5"
   },
   "outputs": [],
   "source": [
    "# !pip install streamlit\n",
    "import os\n",
    "from mlflow import log_metric, log_param, log_artifacts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import mlflow\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment('FluShotLearning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6PH0Xt4WoVr"
   },
   "source": [
    "# Data importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKKz7huoTHuy"
   },
   "source": [
    "via Google Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phBWYZ5T1Dde"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# train_features = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Flu Shot Learning/data/training_set_features.csv\", index_col=\"respondent_id\")\n",
    "# train_labels = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Flu Shot Learning/data/training_set_labels.csv\", index_col=\"respondent_id\")\n",
    "# test_features = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Flu Shot Learning/data/test_set_features.csv\", index_col=\"respondent_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0IhEgz-TLKU"
   },
   "source": [
    "via Uploading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlQY1vSrYlIg"
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(r\"C:\\Development\\FluShotLearning\\data\\training_set_features.csv\", index_col=\"respondent_id\",engine='python')\n",
    "train_labels = pd.read_csv(r\"C:\\Development\\FluShotLearning\\data\\training_set_labels.csv\", index_col=\"respondent_id\", engine='python')\n",
    "test_features = pd.read_csv(r\"C:\\Development\\FluShotLearning\\data\\test_set_features.csv\", index_col=\"respondent_id\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "uqGb1i4_cTXi",
    "outputId": "62624264-9b77-4fc5-a32b-421e8084856c"
   },
   "outputs": [],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5NvU7HkdQpv",
    "outputId": "b0c421d4-b2db-48ab-957a-72f1632a1a90"
   },
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6rR15yAeKzR",
    "outputId": "28514de8-e275-4e64-8183-22431429cd0c"
   },
   "outputs": [],
   "source": [
    "train_features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "bDnk1Q3YeWaK",
    "outputId": "ffd3d304-95eb-498d-d916-2326e1bb4dea"
   },
   "outputs": [],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4xB7RtLe-uM",
    "outputId": "5cdb9936-332e-4982-f844-8736ee7a1c87"
   },
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxRDNZj_vSNH"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "mJM4KbPhla-D",
    "outputId": "63216b79-db58-45e2-f26f-f295e1c8ad9f"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "n_obs = train_labels.shape[0]\n",
    "\n",
    "(train_labels['h1n1_vaccine']\n",
    "    .value_counts()\n",
    "    .div(n_obs)\n",
    "    .plot(kind='bar', stacked=True,title=\"Proportion of H1N1 Vaccine\", color=['steelblue', 'red'],ax=ax[0]))\n",
    "\n",
    "ax[0].set_ylabel(\"h1n1_vaccine\")\n",
    "\n",
    "(train_labels['seasonal_vaccine']\n",
    "    .value_counts()\n",
    "    .div(n_obs)\n",
    "    .plot(kind='bar', stacked=True,title=\"Proportion of Seasonal Vaccine\", color=['steelblue', 'red'],ax=ax[1]))\n",
    "\n",
    "ax[1].set_ylabel(\"seasonal_vaccine\")\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "o90yqHVuuHat",
    "outputId": "708b3bc5-e507-408d-eb6b-7730601ee762"
   },
   "outputs": [],
   "source": [
    "joined_df = train_features.join(train_labels)\n",
    "print(joined_df.shape)\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kP16MAECr9h0"
   },
   "outputs": [],
   "source": [
    "def vaccination_rate_plot(col, target, data, ax=None):\n",
    "    \"\"\"Stacked bar chart of vaccination rate for `target` against \n",
    "    `col`. \n",
    "    \n",
    "    Args:\n",
    "        col (string): column name of feature variable\n",
    "        target (string): column name of target variable\n",
    "        data (pandas DataFrame): dataframe that contains columns \n",
    "            `col` and `target`\n",
    "        ax (matplotlib axes object, optional): matplotlib axes \n",
    "            object to attach plot to\n",
    "    \"\"\"\n",
    "    counts = (joined_df[[target, col]]\n",
    "                  .groupby([target, col])\n",
    "                  .size()\n",
    "                  .unstack(target)\n",
    "             )\n",
    "    group_counts = counts.sum(axis='columns')\n",
    "    props = counts.div(group_counts, axis='index')\n",
    "\n",
    "    props.plot(kind=\"barh\", stacked=True, ax=ax)\n",
    "    ax.invert_yaxis()\n",
    "    ax.legend().remove()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eT1GC2pDsAnL",
    "outputId": "e8ec8c3e-702d-4dc2-9e13-41c444787af0"
   },
   "outputs": [],
   "source": [
    "cols_to_plot = train_features.columns.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    len(cols_to_plot), 2, figsize=(9,len(cols_to_plot)*2.5)\n",
    ")\n",
    "for idx, col in enumerate(cols_to_plot):\n",
    "    vaccination_rate_plot(\n",
    "        col, 'h1n1_vaccine', joined_df, ax=ax[idx, 0]\n",
    "    )\n",
    "    vaccination_rate_plot(\n",
    "        col, 'seasonal_vaccine', joined_df, ax=ax[idx, 1]\n",
    "    )\n",
    "    \n",
    "ax[0, 0].legend(\n",
    "    loc='lower center', bbox_to_anchor=(0.5, 1.05), title='h1n1_vaccine'\n",
    ")\n",
    "ax[0, 1].legend(\n",
    "    loc='lower center', bbox_to_anchor=(0.5, 1.05), title='seasonal_vaccine'\n",
    ")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rj0N8-biAaBv"
   },
   "source": [
    "For correlation between features and variable we should use Cramer's V, or Theil's u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiPfLDS6BabA"
   },
   "outputs": [],
   "source": [
    "def histograms_numeric_columns(df, numerical_columns):\n",
    "    '''\n",
    "    Takes df, numerical columns as list\n",
    "    Returns a group of histagrams\n",
    "    '''\n",
    "    f = pd.melt(df, value_vars=numerical_columns) \n",
    "    g = sns.FacetGrid(f, col='variable',  col_wrap=4, sharex=False, sharey=False)\n",
    "    g = g.map(sns.distplot, 'value')\n",
    "    return g\n",
    "\n",
    "def heatmap_numeric_w_dependent_variable(df, dependent_variable):\n",
    "    '''\n",
    "    Takes df, a dependant variable as str\n",
    "    Returns a heatmap of all independent variables' correlations with dependent variable \n",
    "    '''\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    g = sns.heatmap(df.corr()[[dependent_variable]].sort_values(by=dependent_variable), \n",
    "                    annot=True, \n",
    "                    cmap='coolwarm', \n",
    "                    vmin=-1,\n",
    "                    vmax=1) \n",
    "    return g\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    '''\n",
    "    Returns cramers_v for 2 categorical features\n",
    "    '''\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return round(np.sqrt(phi2corr/min((kcorr-1),(rcorr-1))),3)\n",
    "\n",
    "def heatmap_categorical_columns_w_dependant_categorical(df, dependent_variable, columns):\n",
    "    '''\n",
    "    Takes df, a dependant variable as str\n",
    "    Returns a heatmap of catecorical columns cramers_v with dependent variable \n",
    "    '''\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    corrM = [cramers_v(df[dependent_variable], df[column]) for column in columns]\n",
    "    corr = pd.DataFrame(corrM, index=columns, columns=[dependent_variable])\n",
    "    ax = sns.heatmap(corr,\n",
    "            annot=True,\n",
    "            cmap='coolwarm', \n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "           )\n",
    "    ax.set_title(\"Cramer V Correlation between Variables\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2lS0PJ8fE0-T",
    "outputId": "5ba4040c-87db-43a7-c222-e4eb7ff02151"
   },
   "outputs": [],
   "source": [
    "joined_df.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zjMrO0haEhc6",
    "outputId": "28c872d8-fbd9-4b9f-e508-9b108a4269c5"
   },
   "outputs": [],
   "source": [
    "categorical_cols = list(set(joined_df.columns.values.tolist()))\n",
    "len(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "28WcTqKGFYwC",
    "outputId": "c81162a1-7ad9-49e1-fd3a-dca816a407af"
   },
   "outputs": [],
   "source": [
    "heatmap_categorical_columns_w_dependant_categorical(joined_df, 'h1n1_vaccine' , categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "I_KFqlbDF917",
    "outputId": "8a5f7860-03df-48af-de1d-fa49baf7b4c5"
   },
   "outputs": [],
   "source": [
    "heatmap_categorical_columns_w_dependant_categorical(joined_df, 'seasonal_vaccine' , categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSIk90B-3PND"
   },
   "outputs": [],
   "source": [
    "corr = joined_df.corr()\n",
    "\n",
    "corrM_h1n1 = [cramers_v(joined_df['h1n1_vaccine'], joined_df[column]) for column in categorical_cols]\n",
    "corr_h1n1 = pd.DataFrame(corrM_h1n1, index=categorical_cols, columns=['h1n1_vaccine'])\n",
    "\n",
    "\n",
    "corrM_seasonal = [cramers_v(joined_df['seasonal_vaccine'], joined_df[column]) for column in categorical_cols]\n",
    "corr_seasonal = pd.DataFrame(corrM_seasonal, index=categorical_cols, columns=['seasonal_vaccine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6-oNuK2d49wE",
    "outputId": "81f7270e-b123-495d-f92e-45d2309a6a6e"
   },
   "outputs": [],
   "source": [
    "print(len(corr_h1n1))\n",
    "print(corr_h1n1)\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "ax1 = sns.heatmap(corr_h1n1, linewidths=1,\n",
    "            cmap='coolwarm', vmax=1, vmin=-1, square=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YvUlzGvx49l8",
    "outputId": "966fe85b-6fb9-428b-cd44-8c6104782f7a"
   },
   "outputs": [],
   "source": [
    "print(len(corr_seasonal))\n",
    "print(corr_seasonal)\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "ax1 = sns.heatmap(corr_seasonal, linewidths=1,\n",
    "            cmap='coolwarm', vmax=1, vmin=-1, square=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8uHVe41vzds"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "It seems that in categorical data, when we observe a feature against a label, when there is **little to no difference between category classes** (e.g. h1n1 and sex) we can safely **assume there is low or zero correlation**.\n",
    "\n",
    "**Should there be a divided approach for the two labels?** ;since there are features which can be presumed to have no correlation for one label, and some correlation for another label, e.g. include some features for predicting h1n1 vaccination probability whilst excluding the same for seasonal vaccination?\n",
    "\n",
    "There is some correlation between labels for the same feature, suggesting that feature is universally accepted as a criteria for prediction, whether feature for one label has same distribution, or scaled/standardized distribution for the second label, e.g. *employment industry* and *employment occupation*\n",
    "\n",
    "We should reduce number of features based on correlation to the labels appropriately and seperately, and after that do some prediction according to the label correlation or how h1n1 vaccination is related to the seasonal vaccination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmtVgxHzqZ1c"
   },
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKaXsJhXo7ik"
   },
   "source": [
    "## Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gaW_ITa0RqL"
   },
   "outputs": [],
   "source": [
    "# # find unique elements per column\n",
    "# unique_elements_per_column={i:train_features[i].unique() for i in train_features.columns}\n",
    "# print(unique_elements_per_column)\n",
    "\n",
    "# # create possible category combinations\n",
    "# category_pairs=list(itertools.combinations(train_features.columns,len(train_features.columns)))\n",
    "# #print(len(category_pairs[0]))\n",
    "\n",
    "# # find all possible combinations by category pair\n",
    "# all_possible_combinations=[list(itertools.product(unique_elements_per_column[i[0]],unique_elements_per_column[i[1]])) for i in category_pairs]\n",
    "# all_possible_combinations\n",
    "# # sum of all possible combinations by category pair\n",
    "# sum_combinations=[len(i) for i in all_possible_combinations]\n",
    "# df_out=pd.DataFrame(columns=['all_possible_combinations'],index=category_pairs,data=sum_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVaDKO-g0W4y"
   },
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xb96RI5EkbWp"
   },
   "outputs": [],
   "source": [
    "# print(\"Before dropping nan's\",train_features.shape)\n",
    "# train_features.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n",
    "# print(\"After dropping nan's\", train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fuhgwSRyZPI-",
    "outputId": "3a941a7c-0c56-4c87-c0dc-c479d34f2fdc"
   },
   "outputs": [],
   "source": [
    "multiple_nan_rows_to_drop = 9\n",
    "all_rows = train_features.shape[1] + train_labels.shape[1]\n",
    "\n",
    "print(train_features.shape)\n",
    "merged_df = pd.merge(train_features, train_labels, left_index=True, right_index=True)\n",
    "merged_df = merged_df.dropna(thresh = all_rows - multiple_nan_rows_to_drop)\n",
    "\n",
    "merged_df.shape\n",
    "\n",
    "train_labels = merged_df[[\"h1n1_vaccine\", \"seasonal_vaccine\"]]\n",
    "train_features = merged_df.drop(columns=[\"h1n1_vaccine\", \"seasonal_vaccine\"])\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pZXNepodpKq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be-PUGAH0frX"
   },
   "source": [
    "Since there is only **approximately 1/4 of the data left** when **dropping missing values**, we should devise some different strategy to handling missing values, perhaps assume **predictions** based on **correlation between features**, or **replacing them with most common values** (mean, median, mode) but that has some drawbacks in bias. Maybe we should implement a strategy where the distribution is kept in place while using the best prediction method based on feature correlation.\n",
    "\n",
    "Data for some features are label encoded, they need to be one-hot encoded or target encoded, see https://towardsdatascience.com/handling-categorical-data-the-right-way-9d1279956fc6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZEZAjMgMLJ3"
   },
   "outputs": [],
   "source": [
    "threshold = 0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWPQ952JWuLj",
    "outputId": "6df09627-3d99-41d7-a265-b8746e48d4d9"
   },
   "outputs": [],
   "source": [
    "h1n1_vaccine_columns_to_keep = corr_h1n1.loc[abs(corr_h1n1['h1n1_vaccine']) >= threshold].index.tolist()\n",
    "h1n1_vaccine_columns_to_keep.remove('h1n1_vaccine')\n",
    "len(h1n1_vaccine_columns_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnCDeRMjY1_s",
    "outputId": "e6ef679c-424e-456f-94be-6df3d9baa9a3"
   },
   "outputs": [],
   "source": [
    "seasonal_vaccine_columns_to_keep = corr_seasonal.loc[abs(corr_seasonal['seasonal_vaccine']) >= threshold].index.tolist()\n",
    "seasonal_vaccine_columns_to_keep.remove('seasonal_vaccine')\n",
    "len(seasonal_vaccine_columns_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0y7qeeWHb89t"
   },
   "source": [
    "If the chosen approach is to **predict the labels together** with same columns, the columns to keep should be intersected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zzi7wm-obaTb",
    "outputId": "2ccf46b0-17db-4343-8e9f-d29118d93b57"
   },
   "outputs": [],
   "source": [
    "h1n1_vaccine_columns_to_keep = set(h1n1_vaccine_columns_to_keep)\n",
    "seasonal_vaccine_columns_to_keep = set(seasonal_vaccine_columns_to_keep)\n",
    "\n",
    "intersecting_columns = list(h1n1_vaccine_columns_to_keep.union(seasonal_vaccine_columns_to_keep))\n",
    "intersecting_columns.remove('h1n1_vaccine')\n",
    "intersecting_columns.remove('seasonal_vaccine')\n",
    "print(intersecting_columns)\n",
    "print(len(intersecting_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t9bgsp9-tLgQ",
    "outputId": "7f31e5c7-6ffe-4208-df6b-5626d566ac50"
   },
   "outputs": [],
   "source": [
    "train_features = train_features[intersecting_columns]\n",
    "test_features = test_features[intersecting_columns]\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9P-gx-5JO3P"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "RANDOM_SEED = 6    # Set a random seed for reproducibility!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCehZ5LVVQH6"
   },
   "source": [
    "## Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADx-IPVKle4P",
    "outputId": "b13fc354-b141-463a-fbcd-bfa84f9ca212"
   },
   "outputs": [],
   "source": [
    "missing_values_df = []\n",
    "\n",
    "train_features.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fvr8XFeUihDd",
    "outputId": "4f657aad-8a9f-45df-d660-63861395ac8e"
   },
   "outputs": [],
   "source": [
    "for col in train_features:\n",
    "  print(col, \" \", train_features[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzPm3PL-xrvN"
   },
   "source": [
    "Replacing employee industry and occupation with some other value if they are not employed, other than NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HjBf8-f9qbTA",
    "outputId": "88a62bdd-5345-4db0-d91a-19b077647fd3"
   },
   "outputs": [],
   "source": [
    "if \"employment_status\" in train_features.columns.tolist():\n",
    "  employment_status = train_features[train_features[\"employment_status\"] == 'Unemployed']\n",
    "  employment_status = employment_status[['employment_status','employment_industry','employment_occupation']]\n",
    "  for col in employment_status:\n",
    "    print(col, \" \", employment_status[col].unique())\n",
    "  print(employment_status.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7qTvDnbxA1t"
   },
   "outputs": [],
   "source": [
    "if \"employment_status\" in train_features.columns.tolist():\n",
    "  if \"employment_industry\" in train_features.columns.tolist():\n",
    "    train_features.loc[train_features['employment_status'] == 'Unemployed', 'employment_industry'] = 'unemployed'\n",
    "    test_features.loc[test_features['employment_status'] == 'Unemployed', 'employment_industry'] = 'unemployed'\n",
    "    \n",
    "    train_features.loc[train_features['employment_status'] == 'Not in Labor Force', 'employment_industry'] = 'not_in_labor_force'\n",
    "    test_features.loc[test_features['employment_status'] == 'Not in Labor Force', 'employment_industry'] = 'not_in_labor_force'\n",
    "\n",
    "  if \"employment_occupation\" in train_features.columns.tolist():\n",
    "    train_features.loc[train_features['employment_status'] == 'Unemployed', 'employment_occupation'] = 'unemployed'\n",
    "    test_features.loc[test_features['employment_status'] == 'Unemployed', 'employment_occupation'] = 'unemployed'\n",
    "\n",
    "    train_features.loc[train_features['employment_status'] == 'Not in Labor Force', 'employment_occupation'] = 'not_in_labor_force'\n",
    "    test_features.loc[test_features['employment_status'] == 'Not in Labor Force', 'employment_occupation'] = 'not_in_labor_force'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffbrEs350zC_"
   },
   "source": [
    "Replacing Nan with 0 in health insurance, supposedly people didn't input health insurance because they don't have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOchc4tdPtpc"
   },
   "outputs": [],
   "source": [
    "train_features['health_insurance'] = train_features['health_insurance'].fillna(0)\n",
    "test_features['health_insurance'] = test_features['health_insurance'].fillna(0)\n",
    "\n",
    "train_features['income_poverty'] = train_features['income_poverty'].fillna('> $75,000')\n",
    "test_features['income_poverty'] = test_features['income_poverty'].fillna('> $75,000')\n",
    "\n",
    "train_features['doctor_recc_h1n1'] = train_features['doctor_recc_h1n1'].fillna(0)\n",
    "test_features['doctor_recc_h1n1'] = test_features['doctor_recc_h1n1'].fillna(0)\n",
    "\n",
    "train_features['rent_or_own'] = train_features['rent_or_own'].fillna('other')\n",
    "test_features['rent_or_own'] = test_features['rent_or_own'].fillna('other')\n",
    "\n",
    "# train_features['doctor_recc_seasonal'] = train_features['doctor_recc_seasonal'].fillna(1)\n",
    "# test_features['doctor_recc_seasonal'] = test_features['doctor_recc_seasonal'].fillna(1)\n",
    "\n",
    "train_features['employment_status'] = train_features['employment_status'].fillna('Unemployed')\n",
    "test_features['employment_status'] = test_features['employment_status'].fillna('Unemployed')\n",
    "\n",
    "# train_features['marital_status'] = train_features['marital_status'].fillna('Married')\n",
    "# test_features['marital_status'] = test_features['marital_status'].fillna('Married')\n",
    "\n",
    "# train_features['marital_status'] = train_features['marital_status'].fillna('other')\n",
    "# test_features['marital_status'] = test_features['marital_status'].fillna('other')\n",
    "\n",
    "# train_features['employment_status'] = train_features['employment_status'].fillna('Unemployed')\n",
    "# test_features['employment_status'] = test_features['employment_status'].fillna('Unemployed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UpHwBbfvLAW",
    "outputId": "8caf7261-5530-477d-ebce-06cbb64c1be0"
   },
   "outputs": [],
   "source": [
    "for col in train_features:\n",
    "  print(col, \" \", train_features[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rnM3tnvsXxsy",
    "outputId": "e918afda-4232-4a50-9b3f-7748f37199cc"
   },
   "outputs": [],
   "source": [
    "# if \"doctor_recc_h1n1\" in train_features.columns.tolist():\n",
    "#   employment_status = train_features[train_features[\"employment_status\"].isna()]\n",
    "#   employment_status = employment_status[['employment_status','age_group','education']]\n",
    "#   for col in employment_status:\n",
    "#     print(col, \" \", employment_status[col].unique())\n",
    "#   print(employment_status.shape)\n",
    "  \n",
    "#print(employment_status['health_insurance'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Jx-sZ55yqvF",
    "outputId": "0dc605a8-8173-4cb6-a848-c7ac9744b0df"
   },
   "outputs": [],
   "source": [
    "train_features.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode_train_features = {}\n",
    "# mode_test_features = {}\n",
    "\n",
    "# mode_train_features ['employment_occupation'] = train_features['employment_occupation'].value_counts().nlargest(10).iloc[[1]].index.values[0]\n",
    "# mode_test_features ['employment_occupation']= test_features['employment_occupation'].value_counts().nlargest(10).iloc[[1]].index.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features['employment_occupation'].fillna(mode_train_features['employment_occupation'])\n",
    "# test_features['employment_occupation'].fillna(mode_test_features['employment_occupation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2buyvBQMVo2",
    "outputId": "528af65c-3181-4790-b455-1a33fc8559e3"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def unique_non_null(s):\n",
    "    return s.dropna().unique()\n",
    "\n",
    "#print(train_features.columns.tolist())\n",
    "nominal_not_encoded_columns = ['hhs_geo_region','employment_status', 'employment_industry', 'employment_occupation', 'marital_status', 'rent_or_own', 'sex', 'census_msa', 'race']\n",
    "nominal_two_var_not_encoded_columns = []\n",
    "# encoding 2 value nominal variables into 0 and 1 instead of making 2 seperate variables in pd.get_dummies\n",
    "label_encoder_nominal = LabelEncoder()\n",
    "for col in nominal_not_encoded_columns:\n",
    "  # Key Error handling\n",
    "  if col in train_features.columns.tolist():\n",
    "  #print(train_features[col].unique())\n",
    "  #print(len(train_features[col].unique()))\n",
    "    if(len(unique_non_null(train_features[col])) == 2):\n",
    "      train_features[col] = label_encoder_nominal.fit_transform(train_features[col])\n",
    "      test_features[col] = label_encoder_nominal.fit_transform(test_features[col])\n",
    "      nominal_two_var_not_encoded_columns.append(col)\n",
    "      print(train_features[col])\n",
    "\n",
    "nominal_two_plus_var_not_encoded_columns = list(set(nominal_not_encoded_columns) - set(nominal_two_var_not_encoded_columns))\n",
    "print(nominal_two_plus_var_not_encoded_columns)\n",
    "\n",
    "# Key Error handling\n",
    "nominal_two_plus_var_not_encoded_columns_intersection = set(nominal_two_plus_var_not_encoded_columns).intersection(set(train_features.columns.tolist()))\n",
    "\n",
    "if(len(nominal_two_plus_var_not_encoded_columns_intersection) != 0):\n",
    "  train_features = pd.get_dummies(train_features, prefix=nominal_two_plus_var_not_encoded_columns_intersection, columns=nominal_two_plus_var_not_encoded_columns_intersection)\n",
    "  train_features.columns.tolist()\n",
    "  test_features = pd.get_dummies(test_features, prefix=nominal_two_plus_var_not_encoded_columns_intersection, columns=nominal_two_plus_var_not_encoded_columns_intersection)\n",
    "  train_features.columns.tolist()\n",
    "\n",
    "# TO DO - label encode below column\n",
    "ordinal_not_encoded_columns = ['education', 'age_group', 'income_poverty']\n",
    "label_encoder = LabelEncoder()\n",
    "for col in ordinal_not_encoded_columns:\n",
    "  # Key Error handling\n",
    "  if col in train_features.columns.tolist():\n",
    "    train_features[col] = label_encoder.fit_transform(train_features[col])\n",
    "    test_features[col] = label_encoder.fit_transform(test_features[col])\n",
    "    print(train_features[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_m10HiAiEuwU",
    "outputId": "e642b6fd-4fff-488c-a039-62c32a2cf912"
   },
   "outputs": [],
   "source": [
    "ordinal_values = [1, 2, 3, 4, 5, np.nan]\n",
    "ordinal_cols = []\n",
    "for col in train_features:\n",
    "  if train_features[col].isin(ordinal_values).all():\n",
    "    ordinal_cols.append(col)\n",
    "\n",
    "ordinal_cols.append('h1n1_concern')\n",
    "ordinal_cols.append('h1n1_knowledge')\n",
    "ordinal_cols.append('education')\n",
    "ordinal_cols.append('income_poverty')\n",
    "ordinal_cols.append('age_group')\n",
    "\n",
    "print('Ordinal columns are :', ordinal_cols)\n",
    "\n",
    "\n",
    "nominal_values = [0, 1, np.nan]\n",
    "nominal_values_notencoded = [type(str), type(str), np.nan]\n",
    "nominal_cols = []\n",
    "for col in train_features:\n",
    "  if (train_features[col].isin(nominal_values).all()) :\n",
    "    nominal_cols.append(col)\n",
    "\n",
    "print('Nominal columns are :', nominal_cols)\n",
    "\n",
    "nominal_cols.append('hhs_geo_region')\n",
    "nominal_cols.append('employment_industry')\n",
    "nominal_cols.append('employment_occupation')\n",
    "nominal_cols.append('employment_status')\n",
    "nominal_cols.append('marital_status')\n",
    "nominal_cols.append('rent_or_own')\n",
    "nominal_cols.append('race')\n",
    "\n",
    "categorical_cols = ordinal_cols + nominal_cols\n",
    "categorical = list(set(categorical_cols).intersection(set(train_features.columns.tolist())))\n",
    "print(categorical_cols)\n",
    "\n",
    "numerical_cols = list(set(train_features.columns.tolist()) - set(categorical_cols))\n",
    "#numerical_cols = train_features.columns.tolist() - categorical_cols\n",
    "print(numerical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEno-ZHGB0FC",
    "outputId": "f8c7e04d-0249-4c07-fb20-315fe43373c7"
   },
   "outputs": [],
   "source": [
    "for col in train_features:\n",
    "  print(col, \" \", train_features[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rM3CJ4Xn90i7"
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmcL6STNZ4Mi"
   },
   "outputs": [],
   "source": [
    "# seas_overall_opinion = [i for i in train_features.columns.tolist() if 'seas' in i]\n",
    "# print(seas_overall_opinion)\n",
    "\n",
    "# train_features[\"seas_overall_opinion\"] = train_features[seas_overall_opinion].prod(axis=1) #sum prod\n",
    "# test_features[\"seas_overall_opinion\"] = test_features[seas_overall_opinion].prod(axis=1) \n",
    "# categorical_cols.append(\"seas_overall_opinion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFeudpeDa47T"
   },
   "outputs": [],
   "source": [
    "# h1n1_overall_opinion = [i for i in train_features.columns.tolist() if 'h1n1' in i]\n",
    "# print(h1n1_overall_opinion)\n",
    "\n",
    "# train_features[\"h1n1_overall_opinion\"] = train_features[h1n1_overall_opinion].prod(axis=1) #sum\n",
    "# test_features[\"h1n1_overall_opinion\"] = test_features[h1n1_overall_opinion].prod(axis=1) #sum\n",
    "# categorical_cols.append(\"h1n1_overall_opinion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvKb-jBL0KtJ"
   },
   "outputs": [],
   "source": [
    "#median_cols = []\n",
    "median_cols = ['opinion_seas_sick_from_vacc','opinion_seas_risk','income_poverty', 'opinion_h1n1_sick_from_vacc', 'h1n1_knowledge', 'h1n1_concern', 'opinion_seas_vacc_effective','household_children', 'opinion_h1n1_vacc_effective', 'opinion_h1n1_risk', 'education']\n",
    "# 'opinion_h1n1_sick_from_vacc', 'h1n1_knowledge', 'h1n1_concern', 'opinion_seas_vacc_effective','household_children', 'opinion_h1n1_vacc_effective', 'age_group','opinion_h1n1_risk', 'education'\n",
    "#'age_group',\n",
    "\n",
    "for col in median_cols:\n",
    "  train_features[f\"{col}_median\"] = (train_features[col] > train_features[col].median()).astype(int)\n",
    "  test_features[f\"{col}_median\"] = (test_features[col] > test_features[col].median()).astype(int)\n",
    "  categorical_cols.append(f\"{col}_median\")\n",
    "\n",
    "mean_cols = []\n",
    "# 'household_children', 'household_adults', 'age_group', 'household_children'\n",
    "\n",
    "for col in mean_cols:\n",
    "  train_features[f\"{col}_mean\"] = (train_features[col] > train_features[col].mean()).astype(int)\n",
    "  test_features[f\"{col}_mean\"] = (test_features[col] > test_features[col].mean()).astype(int)\n",
    "  categorical_cols.append(f\"{col}_mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AoXq7e4GSnaC"
   },
   "outputs": [],
   "source": [
    "# behavioral_cols = [i for i in train_features.columns.tolist() if i.startswith('behavioral')]\n",
    "# print(behavioral_cols)\n",
    "\n",
    "# train_features[\"behavioral\"] = train_features[behavioral_cols].sum(axis=1) #sum prod\n",
    "# test_features[\"behavioral\"] = test_features[behavioral_cols].sum(axis=1)\n",
    "# categorical_cols.append(\"behavioral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQxLdRpzVefK"
   },
   "outputs": [],
   "source": [
    "# train_features[\"behavioral\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1GJWs-Bg_b32",
    "outputId": "4c168798-e5ab-4bb0-aad7-6d312488c5fa"
   },
   "outputs": [],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGyACL3edlKg",
    "outputId": "6e4fe2ed-d56c-43a4-c0bf-2d235127fa7a"
   },
   "outputs": [],
   "source": [
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dgkfw_D2oAS1",
    "outputId": "ea540d3e-5d1e-4464-8166-440e28b107b5"
   },
   "outputs": [],
   "source": [
    "print(categorical_cols)\n",
    "print(numerical_cols)\n",
    "print(len(categorical_cols + numerical_cols))\n",
    "print(len(train_features.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_yruFh8ofj-"
   },
   "outputs": [],
   "source": [
    "categorical_cols = list(set(categorical_cols).intersection(set(train_features.columns.tolist())))\n",
    "numerical_cols = list(set(numerical_cols).intersection(set(train_features.columns.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AfrXgfouod_u",
    "outputId": "7d686a10-483d-4794-c06d-b6ed46bb6244"
   },
   "outputs": [],
   "source": [
    "print(categorical_cols)\n",
    "print(numerical_cols)\n",
    "print(len(categorical_cols + numerical_cols))\n",
    "print(len(train_features.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1Q08FIPjpO9",
    "outputId": "80805783-5daf-45d3-ea34-1f66aeb61da0"
   },
   "outputs": [],
   "source": [
    "print(len(categorical_cols + numerical_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-MD3gspaflU"
   },
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "id": "Ee9mSyDQ3XaM",
    "outputId": "b735c8bc-37df-4a18-f462-86a4c49b1bd1"
   },
   "outputs": [],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_cols.append('household_children')\n",
    "# categorical_cols.append('house_h')\n",
    "cols_to_remove_or_append = ['h1n1_knowledge']\n",
    "\n",
    "\n",
    "categorical_mode_cols = []\n",
    "for element in cols_to_remove_or_append:\n",
    "    categorical_cols.remove(element)\n",
    "    categorical_mode_cols.append(element)\n",
    "\n",
    "categorical_mode_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitute_cols = []\n",
    "for col in categorical_mode_cols:\n",
    "  substitute_cols.append(train_features.columns.get_loc(col))\n",
    "\n",
    "categorical_mode_cols = list(substitute_cols)\n",
    "\n",
    "substitute_cols = []\n",
    "for col in categorical_cols:\n",
    "  substitute_cols.append(train_features.columns.get_loc(col))\n",
    "\n",
    "categorical_cols = substitute_cols\n",
    "\n",
    "substitute_cols = []\n",
    "for col in numerical_cols:\n",
    "  substitute_cols.append(train_features.columns.get_loc(col))\n",
    "\n",
    "numerical_cols = substitute_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8LffbSUJrYG"
   },
   "outputs": [],
   "source": [
    "# chain preprocessing into a Pipeline object\n",
    "# each step is a tuple of (name you chose, sklearn transformer)\n",
    "numeric_preprocessing_steps = Pipeline([\n",
    "    ('median_imputer_numerical', SimpleImputer(strategy='median')), ##### impute first\n",
    "    ('standard_scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "n_neighbors = 51\n",
    "\n",
    "categorical_mode_preprocessing_steps = Pipeline([\n",
    "    #('one_hot_encoding', OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ('mode_imputer_categorical', SimpleImputer(strategy='most_frequent')),\n",
    "    ('min_max_scaler', MinMaxScaler())\n",
    "]) \n",
    "\n",
    "categorical_preprocessing_steps = Pipeline([\n",
    "    #('one_hot_encoding', OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ('knn_imputer_categorical', KNNImputer(missing_values=np.nan, n_neighbors=n_neighbors)),\n",
    "    ('min_max_scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "\n",
    "# create the preprocessor stage of final pipeline\n",
    "# each entry in the transformer list is a tuple of\n",
    "# (name you choose, sklearn transformer, list of columns)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('categorical_mode', categorical_mode_preprocessing_steps, categorical_mode_cols),\n",
    "        (\"categorical\", categorical_preprocessing_steps, categorical_cols),\n",
    "        (\"numeric\", numeric_preprocessing_steps, numerical_cols)\n",
    "    ],\n",
    "    remainder = \"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVdDYjsyTJ8I",
    "outputId": "4263834d-1b4e-4c76-90b3-d40f763b2e28"
   },
   "outputs": [],
   "source": [
    "%pip install xgboost\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "\n",
    "# xgb.XGBClassifier(verbosity=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UDfPqtqhg0l"
   },
   "source": [
    "## Select model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU7q6FrlJuZO"
   },
   "outputs": [],
   "source": [
    "estimator = MultiOutputClassifier(\n",
    "    estimator = xgb.XGBClassifier(verbosity=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KClbSexhJwfk"
   },
   "outputs": [],
   "source": [
    "full_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"clf\", estimator),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3KQrS4cJyLU",
    "outputId": "446d0c43-c4c1-40a3-ce7a-c5632f2964bb"
   },
   "outputs": [],
   "source": [
    "full_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDwejNgQanhL"
   },
   "source": [
    "When choosing params in CV, key must be present in below dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfQRYJnbdr8B",
    "outputId": "3458edf9-74bf-4641-9c8b-c16b4c517532"
   },
   "outputs": [],
   "source": [
    "full_pipeline.get_params().keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4nj3fFjJZQr"
   },
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6dpTMjQJ6ud"
   },
   "outputs": [],
   "source": [
    "test_size=0.65\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_size=test_size,\n",
    "    shuffle=True,\n",
    "    stratify=train_labels,\n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNjpTa8-ZfeC"
   },
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xR1etQ1Anvk"
   },
   "outputs": [],
   "source": [
    "def centerlin(center, step, nosteps, dtype):\n",
    "    minus = np.linspace(center,center - step * nosteps , nosteps +1 , dtype = dtype)\n",
    "    #print(minus)\n",
    "    plus = np.linspace(center, center + step * nosteps, nosteps +1, dtype = dtype)\n",
    "    #print(plus)\n",
    "    listnumbers = list(set(minus).union(set(plus)))\n",
    "    listnumbers.sort()\n",
    "    return listnumbers\n",
    "    \n",
    "\n",
    "def centerlog(center, step, nosteps, dtype):\n",
    "    minus = np.logspace(center,center - step * nosteps , nosteps +1 , dtype = dtype)\n",
    "    #print(minus)\n",
    "    plus = np.logspace(center, center + step * nosteps, nosteps +1, dtype = dtype)\n",
    "    #print(plus)\n",
    "    listnumbers = list(set(minus).union(set(plus)))\n",
    "    listnumbers.sort()\n",
    "    return listnumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REQu-i0VKCN1"
   },
   "outputs": [],
   "source": [
    "params_RFC = {\n",
    "    # 'clf__estimator__n_estimators': list(np.linspace(50, 4000, int((4000-50)/500) + 1, dtype=int)),\n",
    "    'clf__estimator__max_depth': list(np.linspace(1,300, 15, dtype=int)),\n",
    "    'clf__estimator__min_samples_split': list(np.linspace(2,300, 15, dtype=int)),\n",
    "    'clf__estimator__min_samples_leaf': list(np.linspace(1,250, 15, dtype=int)),\n",
    "    'clf__estimator__bootstrap': [True, False],\n",
    "    'clf__estimator__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "params_LR = {\n",
    "    'clf__estimator__C': np.logspace(-12, 12, 50),\n",
    "    'clf__estimator__penalty': ['l1', 'l2'],\n",
    "}\n",
    "\n",
    "params_KNN = {\n",
    "    'clf__estimator__leaf_size': list(np.linspace(1,100, 10, dtype=int)),\n",
    "    'clf__estimator__n_neighbors': list(filter(lambda x: x%2 != 0, list(range(20,60)))),\n",
    "    #'clf__estimator__algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'clf__estimator__p': [1, 2, 3, 4]                                \n",
    "}\n",
    "\n",
    "params_SVC = {\n",
    "    'clf__estimator__C': list(np.logspace(-2, 2, 5)),\n",
    "    'clf__estimator__gamma': list(np.logspace(-2, 2, 5)),\n",
    "    'clf__estimator__kernel': ['linear', 'sigmoid']\n",
    "}\n",
    "\n",
    "params_GNB = {\n",
    "    'clf__estimator__var_smoothing': list(np.logspace(-2, 2, 50)),\n",
    "}\n",
    "\n",
    "params_GB = {\n",
    "    'clf__estimator__learning_rate': list(np.logspace(-5,1, 5)),\n",
    "    'clf__estimator__n_estimators': list(np.linspace(50, 500, 5, dtype=int)),\n",
    "    'clf__estimator__max_depth': list(np.linspace(1, 32, 10, endpoint=True)),\n",
    "    'clf__estimator__min_samples_split': list(np.linspace(0.1, 1.0, 10, endpoint=True)),\n",
    "    'clf__estimator__min_samples_leaf':list(np.linspace(0.1, 0.5, 5, endpoint=True)),\n",
    "    'clf__estimator__max_features':list(np.linspace(1,train_features.shape[1], 5, dtype=int))\n",
    "}\n",
    "\n",
    "params_XGB = {\n",
    "    \"clf__estimator__learning_rate\": list(np.logspace(-5,1, 20)),\n",
    "    \"clf__estimator__max_depth\": list(np.linspace(1, 50, 10, endpoint=True,dtype=int)),\n",
    "    \"clf__estimator__min_child_weight\": list(np.logspace(1, 3, 20, endpoint=True)),\n",
    "    \"clf__estimator__gamma\": list(np.logspace(-3, 3, 20)),\n",
    "    \"clf__estimator__colsample_bytree\": list(np.logspace(-3, 0, 20)),\n",
    "    \"clf__estimator__colsample_bylevel\": list(np.logspace(-2, -1, 20)),\n",
    "    \"clf__estimator__colsample_bynode\": list(np.logspace(-2, -1, 10)),\n",
    "    \"clf__estimator__max_delta_step\": list(np.logspace(-1.2,-0.6,10)),\n",
    "    'clf__estimator__reg_lambda': list(np.logspace(-1,5,20)),\n",
    "    'clf__estimator__reg_alpha': list(np.logspace(0.5,1.5,15))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBBR_sjXhOJc"
   },
   "outputs": [],
   "source": [
    "def determine_params(pipeline):\n",
    "\n",
    "  pipeline_estimator_class = pipeline.steps[1][1].estimator.__class__\n",
    "\n",
    "  RFC = RandomForestClassifier().__class__\n",
    "  LR = LogisticRegression().__class__\n",
    "  KNN = KNeighborsClassifier().__class__\n",
    "  SVM = svm.SVC().__class__\n",
    "  GNB = GaussianNB().__class__\n",
    "  GB = GradientBoostingClassifier().__class__\n",
    "  XGB = xgb.XGBClassifier().__class__\n",
    "\n",
    "  if pipeline_estimator_class == RFC :\n",
    "    params = params_RFC\n",
    "  elif pipeline_estimator_class == LR :\n",
    "    params = params_LR\n",
    "  elif pipeline_estimator_class == KNN :\n",
    "    params = params_KNN\n",
    "  elif pipeline_estimator_class == SVM :\n",
    "    params = params_SVC\n",
    "  elif pipeline_estimator_class == GNB :\n",
    "    params = params_GNB\n",
    "  elif pipeline_estimator_class == GB :\n",
    "    params = params_GB\n",
    "  elif pipeline_estimator_class == XGB :\n",
    "    params = params_XGB\n",
    "  \n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNEwhDGT_qLy"
   },
   "outputs": [],
   "source": [
    "#params = determine_params(full_pipeline)\n",
    "#params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "511nYse52YTM"
   },
   "source": [
    "## Validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaanKCBQ2XhE"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "def validation_curve_for_number_param(param_string, pipeline):\n",
    "  param_range = params[param_string]\n",
    "\n",
    "  #if not all(not isinstance(flag, str) for (flag) in param_range):\n",
    "  #  return\n",
    "\n",
    "\n",
    "  train_scores, test_scores = validation_curve(\n",
    "      pipeline,\n",
    "      X_train,\n",
    "      y_train,\n",
    "      param_name=param_string,\n",
    "      param_range=param_range,\n",
    "      scoring=\"roc_auc\",\n",
    "      n_jobs=-1,\n",
    "      verbose=10\n",
    "  )\n",
    "  train_scores_mean = np.mean(train_scores, axis=1)\n",
    "  train_scores_std = np.std(train_scores, axis=1)\n",
    "  test_scores_mean = np.mean(test_scores, axis=1)\n",
    "  test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "  plt.title(\"Validation Curve with Logistic Regression\")\n",
    "  plt.xlabel(param_string)\n",
    "  plt.ylabel(\"Score\")\n",
    "  plt.xticks(range(len(param_range)), param_range, rotation=45)\n",
    "  plt.ylim(0.75, 1.0)\n",
    "\n",
    "  lw = 2\n",
    "  plt.semilogx(\n",
    "      param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", lw=lw\n",
    "  )\n",
    "  plt.fill_between(\n",
    "      param_range,\n",
    "      train_scores_mean - train_scores_std,\n",
    "      train_scores_mean + train_scores_std,\n",
    "      alpha=0.2,\n",
    "      color=\"darkorange\",\n",
    "      lw=lw,\n",
    "  )\n",
    "  plt.semilogx(\n",
    "      param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", lw=lw\n",
    "  )\n",
    "  plt.fill_between(\n",
    "      param_range,\n",
    "      test_scores_mean - test_scores_std,\n",
    "      test_scores_mean + test_scores_std,\n",
    "      alpha=0.2,\n",
    "      color=\"navy\",\n",
    "      lw=lw,\n",
    "  )\n",
    "  plt.legend(loc=\"best\")\n",
    "  plt.grid(True)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOnVyc_QAH26"
   },
   "outputs": [],
   "source": [
    "# for key in params:\n",
    "#   validation_curve_for_number_param(key, full_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cpWSVUa90Tzz",
    "outputId": "9d31fcba-55e6-46d0-dc5a-0624a5b9ac7b"
   },
   "outputs": [],
   "source": [
    "params_RFC = {\n",
    "    # 'clf__estimator__n_estimators': list(np.linspace(50, 4000, int((4000-50)/500) + 1, dtype=int)),\n",
    "    'clf__estimator__max_depth': list(np.linspace(1,300, 15, dtype=int)),\n",
    "    'clf__estimator__min_samples_split': list(np.linspace(2,300, 15, dtype=int)),\n",
    "    'clf__estimator__min_samples_leaf': list(np.linspace(1,250, 15, dtype=int)),\n",
    "    'clf__estimator__bootstrap': [True, False],\n",
    "    'clf__estimator__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "params_LR = {\n",
    "    'clf__estimator__C': np.logspace(-12, 12, 50),\n",
    "    'clf__estimator__penalty': ['l1', 'l2'],\n",
    "}\n",
    "\n",
    "params_KNN = {\n",
    "    'clf__estimator__leaf_size': list(np.linspace(1,100, 10, dtype=int)),\n",
    "    'clf__estimator__n_neighbors': list(filter(lambda x: x%2 != 0, list(range(20,60)))),\n",
    "    #'clf__estimator__algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'clf__estimator__p': [1, 2, 3, 4]                                \n",
    "}\n",
    "\n",
    "params_SVC = {\n",
    "    'clf__estimator__C': list(np.logspace(-2, 2, 5)),\n",
    "    'clf__estimator__gamma': list(np.logspace(-2, 2, 5)),\n",
    "    'clf__estimator__kernel': ['linear', 'sigmoid']\n",
    "}\n",
    "\n",
    "params_GNB = {\n",
    "    'clf__estimator__var_smoothing': list(np.logspace(-2, 2, 50)),\n",
    "}\n",
    "\n",
    "params_GB = {\n",
    "    'clf__estimator__learning_rate': list(np.logspace(-5,1, 10)),\n",
    "    'clf__estimator__n_estimators': list(np.linspace(50, 500, 5, dtype=int)),\n",
    "    'clf__estimator__max_depth': list(np.linspace(1, 10, 10, endpoint=True)),\n",
    "    'clf__estimator__min_samples_split': list(np.logspace(-3, 0, 10, endpoint=True)),\n",
    "    'clf__estimator__min_samples_leaf':list(np.linspace(0.1, 0.5, 5, endpoint=True)),\n",
    "    'clf__estimator__max_features':list(np.linspace(1,train_features.shape[1], 5, dtype=int))\n",
    "}\n",
    "\n",
    "params_XGB = {\n",
    "    \"clf__estimator__learning_rate\": list(np.linspace(0.05,0.15, 20)),\n",
    "    \"clf__estimator__max_depth\": list(np.linspace(1, 50, 10, endpoint=True,dtype=int)),\n",
    "    # \"clf__estimator__max_depth\": list(np.concatenate((np.linspace(5, 9, 5, endpoint=True, dtype=int),(np.linspace(42, 46, 5, endpoint=True, dtype=int))))), # rollback to previous values 1,\n",
    "    \"clf__estimator__min_child_weight\": list(np.linspace(12, 13, 20, endpoint=True)),\n",
    "    \"clf__estimator__gamma\": list(np.logspace(-3, 3, 20)),\n",
    "    \"clf__estimator__colsample_bytree\": list(np.logspace(-3, 0, 20)) \n",
    "\n",
    "    \n",
    "\n",
    "    #\"clf__estimator__learning_rate\": list(np.logspace(-2,-1,10, endpoint=True)),\n",
    "    #\"clf__estimator__max_depth\": list(np.linspace(1, 50, 6, endpoint=True,dtype=int)),\n",
    "    #\"clf__estimator__min_child_weight\": list(np.logspace(1.5, 2.5, 10, endpoint=True)),\n",
    "    #\"clf__estimator__gamma\": list(np.logspace(0.5, 1.5, 10)),\n",
    "    #\"clf__estimator__colsample_bytree\": list(np.logspace(-1, 0, 5,endpoint=True)),\n",
    "    #\"clf__estimator__colsample_bylevel\": list(np.append(np.logspace(-2, -1, 5),1)),\n",
    "    #\"clf__estimator__colsample_bynode\": list(np.append(np.logspace(-2, -1, 5),1)),\n",
    "    #\"clf__estimator__max_delta_step\": list(np.append(np.logspace(-1.2,-0.6,10),0)),\n",
    "    #'clf__estimator__reg_lambda': list(np.append(np.logspace(1.5,3.25,10),1)),\n",
    "    #'clf__estimator__reg_alpha': list(np.append(np.logspace(0.5,1.5,10),0))\n",
    "}\n",
    "\n",
    "params = determine_params(full_pipeline)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7z1JGkqsB4Np",
    "outputId": "c1cded12-bd5d-4978-a7bc-66abf1c51872"
   },
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-v2JC2rdUZb8"
   },
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zlnaq0GcbXT",
    "outputId": "d38e655f-587a-42a4-8de9-5750cf9fe429"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "\n",
    "# params = \n",
    "\n",
    "n_iter = 300\n",
    "cv = 20\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"Fitting started...\")\n",
    "search = RandomizedSearchCV(full_pipeline, param_distributions=params, verbose=10, n_iter = n_iter, error_score=\"raise\", cv=cv) #error_score=\"raise\"\n",
    "search.fit(X_train, y_train)\n",
    "print(f\"Fitting took {time.time() - t0:0.3f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpsgetpTvPmK"
   },
   "outputs": [],
   "source": [
    "train_best_params = search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hcy5sTY6w767"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import time\n",
    "\n",
    "# t0 = time.time()\n",
    "# print(\"Fitting started...\")\n",
    "# search = GridSearchCV(full_pipeline, param_grid=params, verbose=10)\n",
    "# search.fit(X_train, y_train)\n",
    "# print(f\"Fitting took {time.time() - t0:0.3f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjJWh-QBykiS"
   },
   "outputs": [],
   "source": [
    "#search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xdomtW7PfJwy",
    "outputId": "8c251fd3-cf0a-4a93-8d43-e465ed1dbdbb"
   },
   "outputs": [],
   "source": [
    "preds = search.predict_proba(X_eval)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BWsQ2fLKG3d",
    "outputId": "5b745c46-ed53-4320-ca77-1251acfbe832"
   },
   "outputs": [],
   "source": [
    "print(\"test_probas[0].shape\", preds[0].shape)\n",
    "print(\"test_probas[1].shape\", preds[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "HLcv65oFKTEs",
    "outputId": "16fa156f-2e78-4c52-9fef-2b3c7d6f62d8"
   },
   "outputs": [],
   "source": [
    "y_preds = pd.DataFrame(\n",
    "    {\n",
    "        \"h1n1_vaccine\": preds[0][:, 1],\n",
    "        \"seasonal_vaccine\": preds[1][:, 1],\n",
    "    },\n",
    "    index = y_eval.index\n",
    ")\n",
    "print(\"y_preds.shape:\", y_preds.shape)\n",
    "y_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtvhWNg1Kc6t"
   },
   "outputs": [],
   "source": [
    "def plot_roc(y_true, y_score, label_name, ax):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    ax.plot(fpr, tpr)\n",
    "    ax.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    ax.set_ylabel('TPR')\n",
    "    ax.set_xlabel('FPR')\n",
    "    ax.set_title(\n",
    "        f\"{label_name}: AUC = {roc_auc_score(y_true, y_score):.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "tlN4ecbbKewd",
    "outputId": "9b176b91-540d-448c-d650-f6518c0012c6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3.5))\n",
    "\n",
    "plot_roc(\n",
    "    y_eval['h1n1_vaccine'], \n",
    "    y_preds['h1n1_vaccine'], \n",
    "    'h1n1_vaccine',\n",
    "    ax=ax[0]\n",
    ")\n",
    "plot_roc(\n",
    "    y_eval['seasonal_vaccine'], \n",
    "    y_preds['seasonal_vaccine'], \n",
    "    'seasonal_vaccine',\n",
    "    ax=ax[1]\n",
    ")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJUcyk1THuvE"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTCk4sbhK42_",
    "outputId": "2d750509-a060-4320-aff0-d8565cb6dc5a"
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_eval, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RoLbYpsAK_5t",
    "outputId": "98df73be-8247-4524-d255-9a8f18a93c65"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "train_features = train_features.to_numpy()\n",
    "train_labels = train_labels.to_numpy()\n",
    "\n",
    "# params = {\n",
    "\n",
    "search.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVKXlbgJnJT7"
   },
   "outputs": [],
   "source": [
    "valid_best_params = search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6wHLP1hLKb2"
   },
   "outputs": [],
   "source": [
    "test_probas = search.predict_proba(test_features)\n",
    "test_probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcWf-3qyUkfG"
   },
   "source": [
    "## Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8k0zKiKQLUdm"
   },
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(r\"C:\\Development\\FluShotLearning\\data\\submission_format.csv\", index_col=\"respondent_id\",engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4C_W3UhLVg3"
   },
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rm0eHaNhLuAN"
   },
   "outputs": [],
   "source": [
    "# Make sure we have the rows in the same order\n",
    "np.testing.assert_array_equal(test_features.index.values, \n",
    "                              submission_df.index.values)\n",
    "\n",
    "# Save predictions to submission data frame\n",
    "submission_df[\"h1n1_vaccine\"] = test_probas[0][:, 1]\n",
    "submission_df[\"seasonal_vaccine\"] = test_probas[1][:, 1]\n",
    "\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnIxN0glMEa9"
   },
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "submission_df.to_csv(f'{timestr}.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cX_noJhzsLtl"
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "\n",
    "  mlflow.log_param(\"estimator\", full_pipeline.steps[1][1].estimator.__class__)\n",
    "  mlflow.log_param(\"multiple_nan_rows_to_drop\", multiple_nan_rows_to_drop)\n",
    "  mlflow.log_param(\"threshold\", threshold)\n",
    "  mlflow.log_param(\"params\", params)\n",
    "  mlflow.log_param(\"KNN imputer\", n_neighbors)\n",
    "  mlflow.log_param(\"cv\", cv)\n",
    "  mlflow.log_param(\"n-iterations\", n_iter)\n",
    "  mlflow.log_param(\"train best params\", train_best_params)\n",
    "  mlflow.log_param(\"validation best params\", valid_best_params)\n",
    "  mlflow.log_param(\"test size\", test_size)\n",
    "  mlflow.log_metric(\"h1n1 score\", roc_auc_score(y_eval['h1n1_vaccine'], y_preds['h1n1_vaccine']))\n",
    "  mlflow.log_metric(\"seasonal score\", roc_auc_score(y_eval['seasonal_vaccine'], y_preds['seasonal_vaccine']))     \n",
    "  mlflow.log_metric(\"score\", roc_auc_score(y_eval, y_preds))  \n",
    "  mlflow.log_artifact(f'{timestr}.csv')            \n",
    "\n",
    "  # tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "  # if tracking_url_type_store != \"file\":\n",
    "  #     mlflow.sklearn.log_model(rf, \"model\", registered_model_name=\"RandomForestModel\")\n",
    "  # else:\n",
    "  #     mlflow.sklearn.log_model(rf, \"model\") "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Best with fills",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2eb9bb7b8489a71847c8e6cffcce47f86ead647068efc3784c2edb2e02b0297"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
